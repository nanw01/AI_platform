version: '3.8'

services:
  # API网关 - 处理所有外部请求并路由到对应的模型服务
  api-gateway:
    build:
      context: . # 将构建上下文改为项目根目录
      dockerfile: ./docker/Dockerfile.gateway # Dockerfile路径相对于上下文
    ports:
      - "8000:8000"
    volumes:
      # 卷挂载仍然有用，用于开发时的热重载
      # Dockerfile中COPY . /app/api/gateway，所以本地的api/gateway对应容器内的/app/api/gateway
      - ./api/gateway:/app/api/gateway 
      - ./config:/app/config # Dockerfile中COPY ../../config /app/config
      - ./shared:/app/shared # Dockerfile中COPY ../../shared /app/shared
      - ./static:/app/static # 挂载静态文件目录
    environment:
      - ORCHESTRATOR_URL=http://orchestrator:7000
      - VAD_SERVICE_URL=http://vad-service:7001
      - ASR_SERVICE_URL=http://asr-service:7002
      - LLM_SERVICE_URL=http://llm-service:7003
      - TTS_SERVICE_URL=http://tts-service:7004
      - MEMORY_SERVICE_URL=http://memory-service:7005
      - INTENT_SERVICE_URL=http://intent-service:7006
      # PYTHONPATH 在 Dockerfile中设置为 /app
    networks:
      - ai-network
    depends_on:
      - orchestrator

  # 编排服务
  orchestrator:
    image: python:3.10-slim
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn httpx &&
             cd /app && python -m uvicorn orchestrator.main:app --host 0.0.0.0 --port 7000 --reload"
    volumes:
      - ./orchestrator:/app/orchestrator
      - ./shared:/app/shared
      - ./config:/app/config
    ports:
      - "7000:7000"
    environment:
      - VAD_SERVICE_URL=http://vad-service:7001
      - ASR_SERVICE_URL=http://asr-service:7002
      - LLM_SERVICE_URL=http://llm-service:7003
      - TTS_SERVICE_URL=http://tts-service:7004
      - MEMORY_SERVICE_URL=http://memory-service:7005
      - INTENT_SERVICE_URL=http://intent-service:7006
      - REDIS_HOST=redis
      - RABBITMQ_HOST=rabbitmq
      - PYTHONPATH=/app #确保能找到shared模块
    networks:
      - ai-network
    depends_on:
      - vad-service
      - asr-service
      - llm-service
      - tts-service
      - memory-service
      - intent-service
      - redis
      - rabbitmq

  # VAD服务
  vad-service:
    image: python:3.10-slim 
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn # Add VAD specific deps here (e.g., torch torchaudio onnxruntime)
             && cd /app && python -m uvicorn services.vad_service.main:app --host 0.0.0.0 --port 7001 --reload"
    volumes:
      - ./services/vad_service:/app/services/vad_service
      - ./models/vad:/app/models/vad
      - ./shared:/app/shared
    ports:
      - "7001:7001"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network

  # ASR服务
  asr-service:
    image: python:3.10-slim 
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn # Add ASR specific deps here (e.g., funasr-onnx, sherpa-onnx)
             && cd /app && python -m uvicorn services.asr_service.main:app --host 0.0.0.0 --port 7002 --reload"
    volumes:
      - ./services/asr_service:/app/services/asr_service
      - ./models/asr:/app/models/asr
      - ./shared:/app/shared
    ports:
      - "7002:7002"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network

  # LLM服务
  llm-service:
    image: python:3.10-slim 
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn ollama # Add LLM specific deps here (e.g., transformers for local models)
             && cd /app && python -m uvicorn services.llm_service.main:app --host 0.0.0.0 --port 7003 --reload"
    volumes:
      - ./services/llm_service:/app/services/llm_service
      - ./models/llm:/app/models/llm 
      - ./shared:/app/shared
      # - /var/run/docker.sock:/var/run/docker.sock # If llm-service needs to interact with Dockerized Ollama
      # - ollama-data:/root/.ollama # If running Ollama client and want to persist models downloaded by it to a named volume
    ports:
      - "7003:7003"
      # - "11434:11434" # If you intend to run an Ollama server *inside* this llm-service container.
                       # Alternatively, run Ollama as a separate service (see below) 
    environment:
      - PYTHONPATH=/app
      # - OLLAMA_HOST=ollama-server # If Ollama runs as a separate service
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          memory: 8G 
    # depends_on:
      # - ollama-server # If Ollama is a separate service

  # Optional: Ollama as a separate service
  # ollama-server:
  #   image: ollama/ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   networks:
  #     - ai-network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all # or specific count
  #             capabilities: [gpu]

  # TTS服务
  tts-service:
    image: python:3.10-slim
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn edgetts_query # Add other TTS deps like DoubaoTTS
             && cd /app && python -m uvicorn services.tts_service.main:app --host 0.0.0.0 --port 7004 --reload"
    volumes:
      - ./services/tts_service:/app/services/tts_service
      - ./models/tts:/app/models/tts
      - ./shared:/app/shared
    ports:
      - "7004:7004"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network

  # Memory服务
  memory-service:
    image: python:3.10-slim
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn mem0ai # Add other memory deps
             && cd /app && python -m uvicorn services.memory_service.main:app --host 0.0.0.0 --port 7005 --reload"
    volumes:
      - ./services/memory_service:/app/services/memory_service
      - ./shared:/app/shared
      # - ./data/memory_persist:/app/data # For local memory persistence
    ports:
      - "7005:7005"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    depends_on:
      - redis

  # Intent服务
  intent-service:
    image: python:3.10-slim
    command: >
      sh -c "pip install --no-cache-dir fastapi uvicorn # Add Intent specific deps
             && cd /app && python -m uvicorn services.intent_service.main:app --host 0.0.0.0 --port 7006 --reload"
    volumes:
      - ./services/intent_service:/app/services/intent_service
      - ./shared:/app/shared
    ports:
      - "7006:7006"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network

  # Redis 服务
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ai-network

  # RabbitMQ 服务
  rabbitmq:
    image: rabbitmq:3-management-alpine
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq/
    networks:
      - ai-network

#不再保留旧的model-a和model-b服务，因为它们的功能将被新的专用服务取代。
#网络配置
networks:
  ai-network:
    driver: bridge

volumes:
  redis-data:
  rabbitmq-data:
  # ollama-data: # For persisting Ollama models if run as a separate service