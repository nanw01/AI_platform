version: '3.8'

services:
  # API网关 - 处理所有外部请求并路由到对应的模型服务
  api-gateway:
    build:
      context: . # 将构建上下文改为项目根目录
      dockerfile: ./docker/Dockerfile.gateway # Dockerfile路径相对于上下文
    ports:
      - "8000:8000"
    volumes:
      # 卷挂载仍然有用，用于开发时的热重载
      # Dockerfile中COPY . /app/api/gateway，所以本地的api/gateway对应容器内的/app/api/gateway
      - ./api/gateway:/app/api/gateway
      - ./config:/app/config # Dockerfile中COPY ../../config /app/config
      - ./shared:/app/shared # Dockerfile中COPY ../../shared /app/shared
      - ./static:/app/static # 挂载静态文件目录
    environment:
      - ORCHESTRATOR_URL=http://orchestrator:7000
      - VAD_SERVICE_URL=http://vad-service:7001
      - ASR_SERVICE_URL=http://asr-service:7002
      - LLM_SERVICE_URL=http://llm-service:7003
      - TTS_SERVICE_URL=http://tts-service:7004
      - MEMORY_SERVICE_URL=http://memory-service:7005
      - INTENT_SERVICE_URL=http://intent-service:7006
      # PYTHONPATH 在 Dockerfile中设置为 /app
    networks:
      - ai-network
    depends_on:
      orchestrator:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # 编排服务
  orchestrator:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir -r /app/orchestrator/requirements.txt &&
             cd /app && python -m uvicorn orchestrator.main:app --host 0.0.0.0 --port 7000 --reload"
    volumes:
      - ./orchestrator:/app/orchestrator
      - ./shared:/app/shared
      - ./config:/app/config
    ports:
      - "7000:7000"
    environment:
      - VAD_SERVICE_URL=http://vad-service:7001
      - ASR_SERVICE_URL=http://asr-service:7002
      - LLM_SERVICE_URL=http://llm-service:7003
      - TTS_SERVICE_URL=http://tts-service:7004
      - MEMORY_SERVICE_URL=http://memory-service:7005
      - INTENT_SERVICE_URL=http://intent-service:7006
      - REDIS_HOST=redis
      - RABBITMQ_HOST=rabbitmq
      - PYTHONPATH=/app #确保能找到shared模块
    networks:
      - ai-network
    depends_on:
      vad-service:
        condition: service_started
      asr-service:
        condition: service_started
      llm-service:
        condition: service_started
      tts-service:
        condition: service_started
      memory-service:
        condition: service_started
      intent-service:
        condition: service_started
      redis:
        condition: service_started
      rabbitmq:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7000/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # VAD服务
  vad-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn # Add VAD specific deps here (e.g., torch torchaudio onnxruntime)
             && cd /app && python -m uvicorn services.vad_service.main:app --host 0.0.0.0 --port 7001 --reload"
    volumes:
      - ./services/vad_service:/app/services/vad_service
      - ./models/vad:/app/models/vad
      - ./shared:/app/shared
    ports:
      - "7001:7001"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7001/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ASR服务
  asr-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn # Add ASR specific deps here (e.g., funasr-onnx, sherpa-onnx)
             && cd /app && python -m uvicorn services.asr_service.main:app --host 0.0.0.0 --port 7002 --reload"
    volumes:
      - ./services/asr_service:/app/services/asr_service
      - ./models/asr:/app/models/asr
      - ./shared:/app/shared
    ports:
      - "7002:7002"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7002/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # LLM服务
  llm-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn ollama # Add LLM specific deps here (e.g., transformers for local models)
             && cd /app && python -m uvicorn services.llm_service.main:app --host 0.0.0.0 --port 7003 --reload"
    volumes:
      - ./services/llm_service:/app/services/llm_service
      - ./models/llm:/app/models/llm
      - ./shared:/app/shared
      # - /var/run/docker.sock:/var/run/docker.sock # If llm-service needs to interact with Dockerized Ollama
      # - ollama-data:/root/.ollama # If running Ollama client and want to persist models downloaded by it to a named volume
    ports:
      - "7003:7003"
      # - "11434:11434" # If you intend to run an Ollama server *inside* this llm-service container.
      # Alternatively, run Ollama as a separate service (see below) 
    environment:
      - PYTHONPATH=/app
      # - OLLAMA_HOST=ollama-server # If Ollama runs as a separate service
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          memory: 8G
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7003/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
      # depends_on:
      # - ollama-server # If Ollama is a separate service

      # Optional: Ollama as a separate service
      # ollama-server:
      #   image: ollama/ollama
      #   ports:
      #     - "11434:11434"
      #   volumes:
      #     - ollama-data:/root/.ollama
      #   networks:
      #     - ai-network
      #   deploy:
      #     resources:
      #       reservations:
      #         devices:
      #           - driver: nvidia
      #             count: all # or specific count
      #             capabilities: [gpu]

      # TTS服务
  tts-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn edgetts_query # Add other TTS deps like DoubaoTTS
             && cd /app && python -m uvicorn services.tts_service.main:app --host 0.0.0.0 --port 7004 --reload"
    volumes:
      - ./services/tts_service:/app/services/tts_service
      - ./models/tts:/app/models/tts
      - ./shared:/app/shared
    ports:
      - "7004:7004"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7004/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Memory服务
  memory-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn mem0ai # Add other memory deps
             && cd /app && python -m uvicorn services.memory_service.main:app --host 0.0.0.0 --port 7005 --reload"
    volumes:
      - ./services/memory_service:/app/services/memory_service
      - ./shared:/app/shared
      # - ./data/memory_persist:/app/data # For local memory persistence
    ports:
      - "7005:7005"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    depends_on:
      redis:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7005/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Intent服务
  intent-service:
    image: python:3.10-slim
    command: >
      sh -c "apt-get update && apt-get install -y curl && 
             pip install --no-cache-dir fastapi uvicorn # Add Intent specific deps
             && cd /app && python -m uvicorn services.intent_service.main:app --host 0.0.0.0 --port 7006 --reload"
    volumes:
      - ./services/intent_service:/app/services/intent_service
      - ./shared:/app/shared
    ports:
      - "7006:7006"
    environment:
      - PYTHONPATH=/app
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7006/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Redis 服务
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 3

  # RabbitMQ 服务
  rabbitmq:
    image: rabbitmq:3-management-alpine
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq/
    networks:
      - ai-network
    healthcheck:
      test: [ "CMD", "rabbitmq-diagnostics", "check_running" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

#网络配置
networks:
  ai-network:
    driver: bridge

volumes:
  redis-data:
  rabbitmq-data: # ollama-data: # For persisting Ollama models if run as a separate service
